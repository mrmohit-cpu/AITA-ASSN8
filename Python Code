import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
import nltk

# Download stopwords if not already done
nltk.download('stopwords')
stop_words = stopwords.words('english') # Use stopwords.words() directly

# Sample text data
corpus = [
    "Mohit is a rash driver.",
    "He is highly competitive and likes to play cricket.",
    "Keyword extraction is a useful task in Natural Language Processing."
]

# Initialize TF-IDF Vectorizer
tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words) 

# Transform the corpus to TF-IDF matrix
tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)

# Get feature names (words)
feature_names = tfidf_vectorizer.get_feature_names_out()

# Extract keywords from each document
def extract_keywords(doc_index, top_n=5):
    """Extract top N keywords from a document."""
    # Get the TF-IDF scores for the document
    tfidf_scores = tfidf_matrix[doc_index].toarray()[0]
    # Pair each word with its TF-IDF score
    word_score_pairs = list(zip(feature_names, tfidf_scores))
    # Sort by score in descending order
    sorted_words = sorted(word_score_pairs, key=lambda x: x[1], reverse=True)
    # Return the top N words
    return [word for word, score in sorted_words[:top_n]]

# Example: Extract top 5 keywords from each document
for i, doc in enumerate(corpus):
    print(f"Document {i+1}:")
    print(f"Top Keywords: {extract_keywords(i)}")
    print("-" * 50)
